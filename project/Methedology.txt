

# **Methodology**

The **Yemen Market Analysis** project employs a comprehensive methodological framework that integrates advanced econometric and spatial analysis techniques to examine the influence of conflict intensity on commodity prices across different regions of Yemen. A key innovation in this project is the development of a **Time-Variant Market Integration Index (TV-MII)**, which quantifies the dynamic degree of market integration over time. This index captures fluctuations in market integration levels due to factors such as conflict intensity, policy changes, and other exogenous shocks, providing valuable insights for policymakers and stakeholders.

The methodology encompasses data collection and preprocessing, econometric modeling (including the construction of the TV-MII and Error Correction Models), spatial econometric analysis, and data preparation for visualization. It leverages robust statistical models, spatial econometric techniques, and parallel processing to ensure rigorous and scalable analyses. The analysis was implemented using the scripts developed and discussed, ensuring that all computational steps were executed as per the designed methodology.

---

## **1. Data Collection and Preprocessing**

Data preprocessing is crucial to ensure the integrity, consistency, and reliability of the analysis. The following steps were systematically applied to prepare the data for analysis, utilizing parameters specified in the configuration file (`config.yaml`).

### **1.1. Data Loading**

The analysis begins with the acquisition of spatial and temporal data encapsulated in GeoJSON files. The primary dataset, `unified_data.geojson`, contains detailed records of commodity prices, conflict intensity, and other relevant variables across multiple regions and time periods in Yemen.

- **Loading Mechanism:** Utilized **GeoPandas** to read GeoJSON files, ensuring efficient handling of geospatial data.

  ```python
  import geopandas as gpd
  gdf = gpd.read_file('unified_data.geojson')
  ```

- **Configuration Management:** Employed a **YAML configuration file** (`config.yaml`) to manage file paths, directories, and analysis parameters, promoting flexibility and scalability.

### **1.2. Data Cleaning and Transformation**

#### **1.2.1. Handling Duplicates**

- **Objective:** Eliminate duplicate records to prevent bias and inaccuracies.
- **Method:**
  - Identified duplicate records based on key grouping columns (`admin1` or `region_id`, `commodity`, `date`, and `exchange_rate_regime`).
  - For duplicates:
    - **Numeric Columns:** Calculated the average (e.g., `usdprice`, `conflict_intensity`).
    - **Non-Numeric Columns:** Retained the first occurrence.
- **Outcome:** A cleaned dataset with consolidated records, ensuring uniqueness per group.

  ```python
  gdf.drop_duplicates(inplace=True)
  ```

#### **1.2.2. Missing Values Handling**

- **Objective:** Address missing values to maintain data integrity.
- **Method:** Employed imputation strategies, such as replacing missing values with median values, to prevent skewed analyses.

  ```python
  from sklearn.impute import SimpleImputer
  imputer = SimpleImputer(strategy='median')
  gdf['usdprice'] = imputer.fit_transform(gdf[['usdprice']])
  ```

#### **1.2.3. Date Processing**

- **Objective:** Ensure correct date formats for time-series analyses.
- **Method:** Converted date columns to datetime objects.

  ```python
  gdf['date'] = pd.to_datetime(gdf['date'], errors='coerce')
  ```

#### **1.2.4. Categorical Data Encoding**

- **Objective:** Prepare categorical variables for modeling.
- **Method:**
  - Transformed categorical variables (e.g., `commodity`, `exchange_rate_regime`) into suitable formats.
  - Created dummy variables where necessary.

  ```python
  gdf = pd.get_dummies(gdf, columns=['exchange_rate_regime'], drop_first=True)
  ```

#### **1.2.5. Unifying Exchange Rate Regimes**

- **Objective:** Simplify analysis by combining specified exchange rate regimes.
- **Method:**
  - **Regimes to Unify:** 'north' and 'south' as per `regimes_to_unify` in the configuration.
  - **New Regime Name:** Assigned 'unified' as per `new_regime_name`.
  - **Implementation:** Replaced specified regimes with 'unified' in the data.

  ```python
  gdf.loc[gdf['exchange_rate_regime'].isin(['north', 'south']), 'exchange_rate_regime'] = 'unified'
  ```

- **Outcome:** Dataset with a unified exchange rate regime, simplifying comparisons.

#### **1.2.6. Commodity Filtering**

- **Objective:** Focus analysis on specific commodities.
- **Method:**
  - **Commodities List:** Used the list provided under `commodities` in the configuration.
  - **Filtering:** Included only the specified commodities in the dataset.

  ```python
  commodities = config['parameters']['commodities']
  gdf = gdf[gdf['commodity'].isin(commodities)]
  ```

- **Outcome:** Dataset containing only relevant commodities for analysis.

### **1.3. Seasonal Adjustment**

- **Objective:** Remove seasonal effects from `usdprice` to focus on underlying trends.
- **Method:**
  - **Frequency:** Set to monthly (`'M'`) as per the `frequency` parameter.
  - **Grouping:** Data grouped by `commodity` and `admin1` (region identifier).
  - **Decomposition:** Applied seasonal decomposition using an additive model with a period of 12.
  - **Adjustment:** Subtracted the seasonal component from `usdprice` to obtain seasonally adjusted prices.
- **Outcome:** Seasonally adjusted `usdprice` values, enhancing trend analysis.

  ```python
  from statsmodels.tsa.seasonal import seasonal_decompose
  decomposition = seasonal_decompose(gdf['usdprice'], model='additive', period=12)
  gdf['usdprice_adjusted'] = gdf['usdprice'] - decomposition.seasonal
  ```

### **1.4. Smoothing**

- **Objective:** Reduce short-term volatility and noise in `usdprice`.
- **Method:**
  - **Moving Average:** Applied a centered moving average with a window size of 3 to the seasonally adjusted `usdprice`.
  - **Grouping:** Performed within each `commodity` and region group.
- **Outcome:** Smoothed `usdprice` series representing long-term trends.

  ```python
  gdf['usdprice_smoothed'] = gdf.groupby(['commodity', 'region_id'])['usdprice_adjusted'].transform(
      lambda x: x.rolling(window=3, min_periods=1, center=True).mean()
  )
  ```

### **1.5. Data Resampling and Alignment**

- **Objective:** Align data to a consistent temporal frequency and prepare for time-series analyses.
- **Method:**
  - **Frequency Alignment:** Resampled the dataset to a consistent temporal frequency (e.g., monthly) as per `frequency`.
  - **Grouping:** Aggregated data by `commodity`, `exchange_rate_regime`, and `region_id`.
- **Outcome:** Uniform dataset ready for time-series and econometric analyses.

  ```python
  gdf.set_index('date', inplace=True)
  gdf = gdf.groupby(['commodity', 'exchange_rate_regime', 'region_id']).resample('M').mean().reset_index()
  ```

---

## **2. Econometric Modeling**

The project integrates multiple econometric models to dissect the relationship between conflict intensity and commodity prices, as well as to construct the **Time-Variant Market Integration Index (TV-MII)**.

### **2.1. Time-Variant Market Integration Index (TV-MII)**

**Objective:** Quantify the dynamic degree of market integration across different regions over time.

**Methodological Approach:**

#### **2.1.1. Market Pair Selection**

- **Objective:** Identify pairs of markets for comparison.
- **Method:**
  - **Base Markets:** Selected key markets (e.g., Aden, Sana'a) as reference points.
  - **Other Markets:** Included all other markets available in the dataset.
  - **Commodity Matching:** Ensured the same commodities are compared between markets.
- **Outcome:** A set of market pairs for each commodity to be analyzed.

#### **2.1.2. Price Differential Calculation**

- **Compute Log Price Differentials between Market Pairs:**

  \[
  \text{Price Differential}_{ijt} = \ln(P_{it}) - \ln(P_{jt})
  \]

  Where:
  - \( P_{it} \) and \( P_{jt} \) are the prices of the same commodity in regions \( i \) and \( j \) at time \( t \).

- **Implementation:**

  ```python
  price_diff = np.log(price_i) - np.log(price_j)
  ```

#### **2.1.3. Rolling Correlation Analysis**

- **Objective:** Assess the co-movement between markets over time.
- **Method:**
  - **Compute Rolling Correlations of Log Price Changes:**

    \[
    \text{Correlation}_{ijt} = \text{Corr}(\Delta \ln P_{it}, \Delta \ln P_{jt}) \text{ over window } [t - w, t]
    \]

    Where:
    - \( \Delta \ln P_{it} \) is the log price change in region \( i \) at time \( t \).
    - \( w \) is the window size (e.g., 12 months).

  - **Interpretation:** Higher rolling correlations indicate stronger market integration during the window.

- **Implementation:**

  ```python
  window_size = 12
  rolling_corr = price_diff_series.rolling(window=window_size).corr()
  ```

- **Considerations:**
  - **Stationarity:** Ensure that the time series of log price changes are stationary to justify the use of correlation coefficients.
  - **Window Length:** Balance between responsiveness and statistical reliability.

#### **2.1.4. Time-Varying Coefficient Models**

- **Objective:** Capture the dynamic relationship between price differentials and market integration.
- **Method:**
  - **Model Specification:**

    \[
    \text{Price Differential}_{ijt} = \beta_t + \gamma_t \times \text{Conflict Intensity}_{ijt} + \epsilon_t
    \]
    \[
    \beta_t = \beta_{t-1} + \eta_t
    \]
    \[
    \gamma_t = \gamma_{t-1} + \zeta_t
    \]

    Where:
    - \( \beta_t \) captures the baseline degree of market integration at time \( t \).
    - \( \gamma_t \) captures the time-varying influence of conflict intensity on market integration.
    - \( \epsilon_t \) and \( \eta_t \) are error terms representing observation and state noise, respectively.

  - **Estimation:** Employed **State-Space Models** with the **Kalman Filter** to estimate the time-varying coefficients \( \beta_t \) and \( \gamma_t \).

- **Implementation:**

  ```python
  from statsmodels.tsa.statespace.mlemodel import MLEModel

  class TVMIIModel(MLEModel):
      def __init__(self, endog, exog):
          super(TVMIIModel, self).__init__(endog, exog=exog, k_states=2, initialization='approximate_diffuse')
          self.ssm['design'] = np.eye(2)
          self.ssm['transition'] = np.eye(2)
          self.ssm['selection'] = np.eye(2)
          self.initialize_state()

      def initialize_state(self):
          self.ssm.initialize_known(np.array([0, 0]), np.eye(2))

      @property
      def start_params(self):
          return np.log([1.0, 1.0, 1.0, 1.0])  # [obs_cov_beta, state_cov_beta, obs_cov_gamma, state_cov_gamma]

      @property
      def param_names(self):
          return ['obs_cov_beta', 'state_cov_beta', 'obs_cov_gamma', 'state_cov_gamma']

      def update(self, params, **kwargs):
          transformed_params = np.exp(params)
          super().update(transformed_params, **kwargs)
          self.ssm['obs_cov', 0, 0] = transformed_params[0]
          self.ssm['state_cov', 0, 0] = transformed_params[1]
          self.ssm['obs_cov', 1, 1] = transformed_params[2]
          self.ssm['state_cov', 1, 1] = transformed_params[3]
  ```

#### **2.1.5. Index Construction**

- **Normalization:**

  - **Formula:**

    \[
    \text{TV-MII}_{ijt} = \frac{\beta_t - \min(\beta)}{\max(\beta) - \min(\beta)}
    \]

  - **Purpose:** Scale the TV-MII to a [0, 1] range for comparability.

- **Aggregation:**

  - **Weighted Averages:** If analyzing multiple commodities or market pairs, aggregated the indices using weighted averages based on trade volumes or market sizes.

  - **Implementation:**

    ```python
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler(feature_range=(0, 1))
    tv_mii_normalized = scaler.fit_transform(beta_t.reshape(-1, 1)).flatten()
    ```

- **Outcome:** The TV-MII provides a normalized measure of market integration, facilitating comparisons across different times and market pairs.

#### **2.1.6. Incorporating Exogenous Variables**

- **Objective:** Assess the impact of conflict intensity on market integration.
- **Method:**
  - **Extended Model:**

    \[
    \text{Price Differential}_{ijt} = \beta_t + \gamma_t \times \text{Conflict Intensity}_{ijt} + \epsilon_t
    \]
    \[
    \beta_t = \beta_{t-1} + \eta_t
    \]
    \[
    \gamma_t = \gamma_{t-1} + \zeta_t
    \]

  - **Estimation:** Extended the state-space model to include exogenous variables (conflict intensity) with time-varying coefficients using the Kalman Filter.

- **Implementation Considerations:**
  - **Model Complexity:** Introducing additional time-varying coefficients increases model complexity. Ensure sufficient data points to reliably estimate these coefficients.
  - **Regularization:** Consider Bayesian estimation or priors to stabilize coefficient estimates if necessary.

#### **2.1.7. Visualization and Analysis**

- **Time Series Plots:** Visualized the TV-MII over time for different market pairs to observe trends and structural breaks.
- **Spatial Plots:** Mapped the TV-MII geographically to identify regional patterns and areas of high or low integration.

---

### **2.2. Error Correction Model (ECM)**

**Purpose:** Capture both short-term dynamics and long-term equilibrium relationships between commodity prices and conflict intensity.

#### **2.2.1. Stationarity Tests**

- **Objective:** Determine the integration order of the time series for `usdprice` and `conflict_intensity`.
- **Method:**
  - **Transformations:** Applied various transformations (original, logarithmic, differenced, log-differenced) to achieve stationarity.
  - **Tests:**
    - **Augmented Dickey-Fuller (ADF) Test:** Assesses the null hypothesis of a unit root (non-stationarity).
    - **Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test:** Assesses the null hypothesis of stationarity.
  - **Significance Level:** Used a 5% significance level as per `stationarity_significance_level`.
  - **Decision Criteria:** Selected the transformation where both ADF and KPSS tests indicate stationarity.
- **Outcome:** Stationary series suitable for cointegration testing.

- **Implementation:**

  ```python
  from statsmodels.tsa.stattools import adfuller, kpss

  def test_stationarity(series):
      result_adf = adfuller(series.dropna())
      result_kpss = kpss(series.dropna(), regression='c')
      return result_adf[1] < 0.05 and result_kpss[1] > 0.05
  ```

#### **2.2.2. Cointegration Tests**

- **Objective:** Test for a long-term equilibrium relationship between `usdprice` and `conflict_intensity`.
- **Method:**
  - **Engle-Granger Two-Step Method:**
    - **First Step:** Estimate a long-run equilibrium relationship using Ordinary Least Squares (OLS).
    - **Second Step:** Test the residuals from the first step for stationarity using the ADF test.
  - **Max Lags:** Used up to 12 lags as per `cointegration_max_lags`.
  - **Significance Level:** Used a 5% significance level as per `cointegration_significance_level`.
- **Outcome:** Identification of cointegrated relationships suitable for ECM estimation.

- **Implementation:**

  ```python
  from statsmodels.tsa.stattools import coint

  score, pvalue, _ = coint(y, x, maxlag=12)
  if pvalue < 0.05:
      print("Cointegration exists")
  ```

#### **2.2.3. ECM Estimation**

- **Objective:** Model both the short-term dynamics and the long-term equilibrium relationship.
- **Method:**
  - **Vector Error Correction Model (VECM):**
    - **Lag Selection:** Used the `lag_periods` parameter set to 2.
    - **Cointegration Rank:** Determined using Johansen cointegration test.
    - **Specification:** Included deterministic terms and exogenous variables.
  - **Estimation:** Fitted the VECM to estimate adjustment coefficients.

- **Outcome:** An ECM capturing the interplay between commodity prices and conflict intensity over time.

- **Implementation:**

  ```python
  from statsmodels.tsa.vector_ar.vecm import VECM

  vecm = VECM(data, k_ar_diff=2, coint_rank=1)
  vecm_fit = vecm.fit()
  ```

#### **2.2.4. Model Diagnostics**

- **Objective:** Validate the ECM's adequacy and robustness.
- **Methods Applied:**
  - **Breusch-Godfrey Serial Correlation LM Test**
  - **ARCH Test for Heteroskedasticity**
  - **Jarque-Bera and Shapiro-Wilk Tests for Normality**
  - **Durbin-Watson Statistic for Autocorrelation**
  - **White's Test for Heteroskedasticity**
  - **Autocorrelation and Partial Autocorrelation Functions (ACF and PACF)**
- **Outcome:** Diagnostic results confirming the validity of the model or indicating areas requiring attention.

- **Implementation:**

  ```python
  from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch, het_white
  from statsmodels.stats.stattools import jarque_bera
  from statsmodels.stats.stattools import durbin_watson

  residuals = vecm_fit.resid
  lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)
  arch_test = het_arch(residuals)
  white_test = het_white(residuals)
  jb_test = jarque_bera(residuals)
  dw_stat = durbin_watson(residuals)

  print(lb_test)
  print(arch_test)
  print(white_test)
  print(jb_test)
  print(dw_stat)
  ```

#### **2.2.5. Impulse Response Functions (IRFs)**

- **Objective:** Analyze the dynamic response of `usdprice` to shocks in `conflict_intensity`.
- **Method:** Computed IRFs from the fitted VECM over a 10-period horizon.
- **Outcome:** Insights into how shocks to conflict intensity affect commodity prices over time.

- **Implementation:**

  ```python
  irf = vecm_fit.irf(10)
  irf.plot(orth=False)
  ```

#### **2.2.6. Granger Causality Tests**

- **Objective:** Determine the direction of causality between `usdprice` and `conflict_intensity`.
- **Method:** Performed Granger causality tests using the time series data, with a maximum lag of 4 as per `granger_max_lags`.
- **Outcome:** Understanding of causal relationships, aiding in model interpretation.

- **Implementation:**

  ```python
  from statsmodels.tsa.stattools import grangercausalitytests

  granger_results = grangercausalitytests(data[['usdprice', 'conflict_intensity']], maxlag=4)
  ```

---

## **3. Spatial Econometric Analysis**

Spatial econometric techniques are pivotal in understanding the geographical interdependencies and spatial spillover effects in the data.

### **3.1. Spatial Weights Matrix Construction**

- **Objective:** Define spatial relationships between regions.
- **Method:**
  - **K-Nearest Neighbors (KNN):** Created spatial weights matrices using KNN with varying `k`.
    - **Initial k:** Started with `initial_k` set to 5.
    - **Maximum k:** Increased up to `max_k` set to 10 if the weights matrix was not fully connected.
  - **Connectivity Check:** Ensured the spatial weights matrix is fully connected.
  - **Threshold Multiplier:** Used `threshold_multiplier` set to 1 for distance-based weights (if applicable).
  - **Self-Neighborhood Exclusion:** Ensured regions do not include themselves as neighbors.
- **Outcome:** Spatial weights matrix representing spatial dependencies.

- **Implementation:**

  ```python
  from libpysal.weights import KNN

  k = 5
  while True:
      w = KNN.from_dataframe(gdf, k=k)
      if w.is_connected:
          break
      k += 1
      if k > 10:
          raise ValueError("Unable to create a fully connected spatial weights matrix with k up to 10.")
  ```

### **3.2. Spatial Lag Models**

- **Objective:** Model spatial relationships considering multicollinearity.
- **Method:**
  - **Spatial Lag Calculation:**

    \[
    \text{Spatial Lag of Price}_{it} = \sum_{j} w_{ij} \times \text{Price}_{jt}
    \]

  - **Ridge Regression with Spatial Lag:** Performed Ridge regression to mitigate multicollinearity and stabilize coefficient estimates.
    - **Independent Variables:** Spatial lag of `usdprice`.
    - **Dependent Variable:** `usdprice`.
    - **Imputation:** Handled missing values using median imputation.
    - **Standardization:** Standardized features using `StandardScaler`.
    - **Modeling:** Performed Ridge regression with `alpha` set to 1.0 as per `ridge_alpha`.

- **Outcome:** Estimated coefficients reflecting spatial dependencies.

- **Implementation:**

  ```python
  from sklearn.linear_model import Ridge
  from sklearn.preprocessing import StandardScaler
  from sklearn.impute import SimpleImputer

  imputer = SimpleImputer(strategy='median')
  scaler = StandardScaler()

  X = w.sparse @ gdf['usdprice']
  X = imputer.fit_transform(X.reshape(-1, 1))
  X = scaler.fit_transform(X)

  y = gdf['usdprice'].values

  model = Ridge(alpha=1.0)
  model.fit(X, y)
  coefficients = model.coef_
  ```

### **3.3. Model Diagnostics**

- **Objective:** Validate the spatial regression model.
- **Methods Applied:**
  - **Variance Inflation Factor (VIF):** Assessed multicollinearity among predictors.
  - **P-Values Estimation:** Approximated p-values for Ridge regression coefficients.
  - **Moran's I:** Calculated to assess spatial autocorrelation in residuals.

    \[
    I = \frac{N}{\sum_{i} \sum_{j} w_{ij}} \cdot \frac{\sum_{i} \sum_{j} w_{ij}(y_i - \overline{y})(y_j - \overline{y})}{\sum_{i} (y_i - \overline{y})^2}
    \]

- **Outcome:** Diagnostics confirming model validity or highlighting issues.

- **Implementation:**

  ```python
  from statsmodels.stats.outliers_influence import variance_inflation_factor
  from esda.moran import Moran

  # Calculate VIF
  vif_data = pd.DataFrame()
  vif_data["feature"] = ["Spatial Lag"]
  vif_data["VIF"] = [variance_inflation_factor(X, 0)]
  print(vif_data)

  # Calculate residuals
  residuals = y - model.predict(X)

  # Calculate Moran's I
  moran = Moran(residuals, w)
  print(f"Moran's I: {moran.I}, p-value: {moran.p_sim}")
  ```

### **3.4. Residual Analysis**

- **Objective:** Analyze residuals for further insights.
- **Method:**
  - Extracted residuals from the regression model.
  - Merged residuals back into the GeoDataFrame.
- **Outcome:** Residuals associated with regions and dates for further analysis.

- **Implementation:**

  ```python
  gdf['residuals'] = residuals
  ```

---

## **4. Data Preparation for Visualization**

Prepared datasets for creating visualizations such as choropleth maps, time series plots, and network graphs.

### **4.1. Choropleth Maps Data Preparation**

- **Objective:** Prepare data for spatial visualizations.
- **Methods:**
  - **Average Prices:** Calculated average `usdprice` per region and date.
  - **Conflict Intensity:** Calculated average `conflict_intensity` per region and date.
  - **Price Changes:** Computed percentage change in `usdprice` over time.
  - **Residuals:** Extracted residuals for mapping.
- **Outcome:** JSON files containing data for generating choropleth maps.

### **4.2. Time Series Data Preparation**

- **Objective:** Prepare time series data for analysis and visualization.
- **Methods:**
  - **Prices Time Series:** Pivoted data to create time series of `usdprice` per commodity and regime.
  - **Conflict Intensity Time Series:** Aggregated conflict intensity over time.
- **Outcome:** JSON files with time series data for prices and conflict intensity.

### **4.3. Network Data Preparation (Flow Maps)**

- **Objective:** Generate data for network graphs illustrating spatial relationships.
- **Methods:**
  - **Flow Data Generation:** Created flow data using the spatial weights matrix, including source and target regions with coordinates.
  - **Spatial Lag of Prices:** Included spatially lagged `usdprice` as weights.
- **Outcome:** JSON file with flow map data for network visualizations.

---

## **5. Parallel Processing and Optimization**

To handle computationally intensive tasks efficiently, the project leverages parallel processing techniques.

- **Multiprocessing:** Utilized Python’s `multiprocessing` and `concurrent.futures` modules to execute multiple analyses concurrently, using up to 8 parallel processes as per `parallel_processes`.

  ```python
  from concurrent.futures import ProcessPoolExecutor
  with ProcessPoolExecutor(max_workers=8) as executor:
      results = list(executor.map(analyze_market_pair, analysis_args))
  ```

- **Caching Mechanisms:** Implemented caching strategies using `joblib` to optimize repeated data loading and processing steps.

  ```python
  from joblib import Memory
  memory = Memory(location='cache_dir', verbose=0)

  @memory.cache
  def load_data(file_path):
      # Data loading logic
      return gpd.read_file(file_path)
  ```

- **Data Optimization:** Simplified geometries and optimized data types to enhance performance and minimize memory usage.

  ```python
  gdf['geometry'] = gdf['geometry'].simplify(tolerance=0.01, preserve_topology=True)
  gdf = gdf.astype({'usdprice': 'float32', 'conflict_intensity': 'float32'})
  ```

- **Resource Management:** Monitored memory usage per process, limiting to 4 GB as per `memory_per_process_gb`.

---

## **6. Result Aggregation and Saving**

All analytical outcomes are systematically aggregated and stored for downstream applications and reporting.

- **JSON Outputs:** Saved results in JSON format with indentation for readability, facilitating ease of access and integration with visualization tools.

  ```python
  import json
  gdf.to_json('results/spatial_analysis_results.json', orient='records', date_format='iso', indent=4)
  ```

- **Logging:** Maintained detailed logs of all processes and analyses, ensuring transparency and facilitating debugging and reproducibility.

  ```python
  import logging
  logging.basicConfig(filename='results/logs/spatial_analysis.log', level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(message)s')
  logger = logging.getLogger(__name__)
  ```

- **File Path Management:** Utilized Python’s `pathlib` for robust and cross-platform file path handling.

  ```python
  from pathlib import Path
  results_dir = Path('results/')
  results_dir.mkdir(parents=True, exist_ok=True)
  ```

---

## **7. Considerations and Assumptions**

- **Data Quality:** Assumed accuracy and representativeness of the provided data. Addressed data quality through preprocessing steps such as handling duplicates and missing values.
- **Stationarity and Cointegration:** Relied on statistical tests (ADF, KPSS, Cointegration tests) to inform model specifications, ensuring that the data meets the assumptions required for time-series analyses.
- **Minimum Observations:**
  - Required at least 20 common dates for price differential analysis (`min_common_dates`).
  - Required at least 5 regions for spatial analysis (`min_regions`).
- **Geographic Accuracy:** Assumed accurate geographic coordinates for spatial analysis. Simplified geometries to optimize performance without significantly compromising spatial accuracy.
- **Model Selection:** Chose models appropriate for data characteristics (e.g., Ridge regression for multicollinearity mitigation in spatial lag models).
- **Resource Management:** Managed computational resources by limiting processes and memory usage per process, ensuring efficient use of available hardware.
- **Configuration Flexibility:** Utilized a YAML configuration file to allow easy adjustment of parameters and paths, enhancing the scalability and adaptability of the analysis pipeline.

---

## **8. Limitations**

- **Data Availability:** Analyses are limited by data completeness and coverage. Missing regions or time periods may affect the robustness of the findings.
- **Model Simplifications:** Models may not capture all complexities, such as non-linear relationships or interactions between multiple exogenous variables.
- **External Factors:** Potentially influential external variables (e.g., policy changes, economic sanctions) may not be included, which could bias the results.
- **Temporal Resolution:** Monthly data may not capture short-term dynamics or rapid changes in conflict intensity and market integration.
- **Spatial Resolution:** Spatial weights based on KNN may not capture all spatial dependencies, especially in regions with irregular geographic distributions.
- **Parallel Processing Constraints:** Limited by computational resources and memory availability, which may restrict the scalability of the analysis for larger datasets or more complex models.
- **Normalization Sensitivity:** The normalization process for TV-MII is sensitive to outliers, which may distort the index if not properly addressed.

